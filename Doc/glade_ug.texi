\input texinfo @c -*-texinfo-*-
@setfilename glade_ug.info
@settitle GLADE User's Guide
@setchapternewpage odd
@syncodeindex fn cp

@titlepage

@title GLADE User's Guide
@subtitle GLADE, GNAT Library for Ada Distributed Environment
@subtitle Version 2.01 (DRAFT)
@subtitle December 16, 1997
@author Laurent Pautet, Samuel Tardieu

@page
@vskip 0pt plus 1filll

@copyright{} Copyright 1997-1998, Free Soft Foundation.

GLADE is free software; you can redistribute it and/or modify it under
terms of the GNU General Public License as published by the Free
Software Foundation; either version 2, or (at your option) any later
version. GNAT is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANT
ABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
License for more details.  You should have received a copy of the GNU
General Public License along with GNAT; see file COPYING. If not, write
to the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.

@end titlepage

@ifinfo
@node Top, Tutorial on the Distributed Systems Annex, (dir), (dir)
@top GLADE User's Guide

GLADE is the GNAT implementation of Distributed Systems Annex.

@menu
* Tutorial on the Distributed Systems Annex::  
* Getting Started With GLADE::  
* Index::                       
@end menu

@end ifinfo

@node Tutorial on the Distributed Systems Annex, Getting Started With GLADE, Top, Top
@chapter Tutorial on the Distributed Systems Annex

@menu
* Introduction To Distributed Systems::  
* The Distributed System Annex::  
* Architecture of A DSA Application::  
* Presentation Of Categorization Pragmas::  
* Pragma Remote_Call_Interface::  
* Pragma Remote_Types::         
* Pragma Shared_Passive::       
* More About Pragmas::          
* Most Features in One Example::  
* A Comparison between CORBA and DSA::  
@end menu

@node Introduction To Distributed Systems, The Distributed System Annex, Tutorial on the Distributed Systems Annex, Tutorial on the Distributed Systems Annex
@section Introduction To Distributed Systems

@menu
* Using OS Network Services::   
* Using A Middleware Environment::  
* Using A Distributed Language::  
@end menu

A distributed system architecture comprises a network of computers and the
software components that execute on the computers. Such architectures are
commonly used to improve the performance, reliability, and reusability of
complex applications. Typically, when there is no shared address space
available to remotely-located components, components must communicate using
some form of message-passing abstraction.

@node Using OS Network Services, Using A Middleware Environment, Introduction To Distributed Systems, Introduction To Distributed Systems
@subsection Using OS Network Services

There are several programming techniques for developing distributed
applications. These applications have traditionally been developed using
network programming interfaces such as sockets. Programmers explicitly
have to perform calls to operating system services, a task that can be
tedious and error-prone. This includes initializing socket connection
and determining peer location, marshaling and unmarshaling data
structures, sending and receiving messages, debugging and testing
several programs at the same time and porting on several platforms to
hide subtle differences between various network interfaces.

Of course, this code can be encapsulated in wrappers to reduce its
complexity but it is clear that most of it can be automatically
generated. Message passing diverts developer's attention from the
application domain. The query and reply scenario is a classical scheme
in distributed applications; using message passing in such a situation
could be compared to using a ``goto'' mechanism in a non-distributed
application.  This is known to be a significant problem in the domain of
modern programming languages. A more robust design would be to use a
structured approach based on procedure call.

In some respects, this network programming issue can be compared to the
multi-threading programming issue. An user can decide to split his code
in several pieces and to multiplex the thread executions himself using a
table-driven model. The scheduling code would be embedded into the user
code. This solution is error-prone and fragile in regard to any future
modification.

@node Using A Middleware Environment, Using A Distributed Language, Using OS Network Services, Introduction To Distributed Systems
@subsection Using A Middleware Environment

A middleware environment is intended to provide high level abstractions
in order to easily develop user applications.  Environments like CORBA
or Distributed Computing Environment (DCE) propose an approach to
develop client/server applications using the Remote Procedure Call model
(RPC). The RPC model is inspired from the query and reply
scheme. Compared to a regular procedure call, arguments are pushed into
a stream along with some data pointing out which remote procedure is to
be used and the stream is transmitted over the network to the
server. The server decodes the stream, does the regular subprogram call,
then put the output parameters into another stream along with the
exception (if any) raised by the subprogram and sends this stream back
to the caller. The caller decodes the stream and raises the exception if
needed.

CORBA provides the same enhancements to the remote procedure model that
object languages provide to classical procedural languages.  This also
includes encapsulation, inheritance, type checking, and
exceptions. These features are offered through an Interface Definition
Language (IDL).

The middleware communication framework provides all the machinery to
perform, somewhat transparently, remote procedure calls or remote object
method invocations. For instance, each CORBA interface communicates
through an Object Request Broker (ORB). A communication subsystem such
as an ORB is intended to allow applications to use objects without being
aware of their underlying message passing implementation. But the user
may also require a large number of complex services to develop the
distributed application. Some of them are definitively needed like a
location service that allows clients to reference remote services via
higher level names instead of a traditional scheme for addressing remote
services involving Internet host addresses and communication port
number. Other services provide domain independent interfaces that are
frequently used by distributed applications like naming services.

If we get back to the multi-thread programming comparison, the
middleware solution is close to what a POSIX library or a language like
Esterel would propose to develop a concurrent application. A middleware
framework like DCE is close to a POSIX library in terms of abstraction
levels. Functionalities are very low-level and very complex. CORBA is
closer to Esterel in terms of development process.  The control part of
the application can be specified in a description language. The
developer then has to fill in automatically generated source code (stub
and skeletons) to call the computation part of the application. The
distribution is a pre-compilation process and the distributed boundaries
are always explicit. Using CORBA, the distributed part is written in IDL
and the core of the application is written in a host language.

@node Using A Distributed Language,  , Using A Middleware Environment, Introduction To Distributed Systems
@subsection Using A Distributed Language

Rather than defining a new language like an IDL, an alternative idea is
to extend a programming language in order to provide distributed
features. The distributed object paradigm provides a more
object-oriented approach to programming distributed systems. The notion
of a distributed object is an extension to the abstract data type that
permits the services provided in the type interface to be called
independently of where the actual service is executed. When combined
with object-oriented features such as inheritance and polymorphism,
distributed objects promote a more dynamic and structured computational
environment for distributed applications.

Ada95 includes a Distributed Systems Annex (DSA) which defines several
extensions allowing a user to write a distributed system entirely in
Ada, using Ada packages as the definition of remote procedure call or
remote method call on distributed objects. Ada95 distributed systems
model is close to Modula-3's one and Java/RMI's one is similar to
Ada95's one. In these languages, the IDL is replaced by a subset of the
language. Therefore, the language supports both remote procedure calls
and remote object method invocations transparently.

A program written in such a language is supposed to communicate with a
program written in the same language, but this restriction gives rise to
several useful consequences. The language can provide more powerful
features because it is not constrained by the common features available
in all host languages. In Ada95, the user will define a specification of
remote services and implement them exactly as he would for ordinary,
non-distributed services. His Ada95 environment will compile them to
produce a stub file (on the caller side) and a skeleton file that
automatically includes the services body (on the receiver
side). Creating objects, obtaining or registering object references or
adapting the object skeleton to the user object implementation are made
transparent because the language environment has a full control on the
development process.

Comparing with multi-thread programming once again, the language
extension solution is equivalent to the solution adopted for tasking
facilities in Ada.  Writing a distributed application is as simple as
writing a concurrent application: there is no binding consideration and
no code to wrap.  The language and its run-time system take care of
every issue that would divert the programmer's attention from the
application domain.

@node The Distributed System Annex, Architecture of A DSA Application, Introduction To Distributed Systems, Tutorial on the Distributed Systems Annex
@section Overview of Ada95 Distributed System Annex

The basic idea of the Distributed Systems Annex (DSA) is to allow a user
to develop his application the same way whether this application is
going to be executed as several programs on a distributed system or as a
whole program on a non-distributed system. The DSA has been designed so
as to minimize the changes required to the source code of a program, in
converting it from an ordinary non-distributed program, to a distributed
program.

The easiest way to start with DSA is certainly to develop the
application on a non-distributed system. Of course, the design of the
application should take into account the fact that some units are going
to be accessed remotely. In order to write an Ada95 distributed program,
it is necessary for the user to categorize certain library level
compilation units of the application program, by inserting
categorization pragmas into their specification. The units which require
categorization are typically those which are called remotely, or ones
which provide types used in remote calls.

Therefore, these units must contain only a restricted set of Ada
entities. For instance, if the distributed system has no shared memory,
shared variables should be forbidden. To ensure such restrictions, the
DSA provides several categorization pragmas in order to reduce the set
of entities one can declare in a given unit.

Of course, you can develop the non-distributed application with your
usual software engineering environment. This is very important to note
that the user needs no specialized tools to develop his/her distributed
application. For instance, he can debug his/her application with his/her
usual debugger. A non-distributed program is not to be confused with a
distributed application composed of only one program. The later is built
with the help of the configuration tool and includes the communication
library.

The last step is to partition and to configure the non-distributed
application into a distributed application, that means multiple
partitions working cooperatively as part of a single Ada program. The
process of mapping the partitions of a program to the nodes in a
distributed system is called configuring the partitions of the
program. This is what GLADE is for.

The distributed version of the user application should work as is, but
even when a program can be built either as a non-distributed or a
distributed program using the same source code, there may still be
differences in program execution between the distributed and
non-distributed versions. These differences will be presented in others
sections.

Developping a non-distributed application in order to distribute it
later on is the natural approach for a novice. Of course, it is not
always possible to write a distributed application as a non-distributed
application. For instance, a client/server application does not belong
to this category because several instances of the client can be active
at the same time. It is very easy to develop such an application using
GLADE and we shall present to the expert how to do that in the following
sections.

@node Architecture of A DSA Application, Presentation Of Categorization Pragmas, The Distributed System Annex, Tutorial on the Distributed Systems Annex
@section Architecture of A Distributed Ada95 Application

A distributed system is an interconnection of one or more processing
nodes and zero or more storage nodes. A distributed program comprises
one or more partitions. A partition is an aggregate of library
units. Partitions communicate through shared data or RPCs. A passive
partition has no thread of control. Only a passive partition can be
configured on a storage node. An active partition has zero or more
threads of control and has to be configured on a processing node.

The library unit is the core component of an Ada95 distributed
application. The user can explicitly assign library units to a
partition. The partitioning is a post-compilation process. The user
identifies at compile time interface packages. These packages are
categorized using pragmas. Each of these pragmas allows to use one of
the following classical paradigms:

@itemize @bullet
@item Remote subprograms:
For the programmer, a remote subprogram call is similar to a regular
subprogram call. Run-time binding using access-to-subprogram types can
also be used with remote subprograms. These remote subprograms are
mostly declared in library units categorized as remote call interface
(RCI).
  
@item Distributed objects:
Remote-pointer particular access types can be defined, which designate
remote objects. When a primitive dispatching operation is invoked on an
object designated by a remote access, a remote call is performed
transparently on the partition on which the object was created. These
distributed objects are declared in library units categorized as remote
types (RT).
  
@item Shared objects:
Global data can be shared between active partitions, providing a
repository similar to a shared memory, a shared file system or a
database. Entry-less protected objects allow safe access and update on
shared objects. This feature is orthogonal to the notion of distributed
objects, which are only accessed through exported services. These shared 
objects are declared in library units categorized as shared passive (SP).

@end itemize

The remotely-called subprograms declared in a library unit categorized
as remote call interface (RCI) or remote types (RT) may be either
statically or dynamically bound. The partition on which a statically
bound remote subprogram is executed can be determine before the
call. This is a regular remote subprogram call. A remote method or a
dereference of access to remote subprogram are dynamically bound remote
calls because the partition on which the remote subprogram is executed
is determined by the parameters of the call.

In the following example, Data_1 and Data_2 are shared passive (SP)
library units. Data_1 is configured on a passive partition mapped on a
storage node. Partition_1 and Partition_2 are active partitions. Note
that under some circumstances, a partition, for instance Partition_2,
can be duplicated. To be duplicated, Unit_2 and Unit_3 which are
configured on Partition_2 have to provide only dynamically bound remote
subprograms. Otherwise, a partition calling a remote subprogram on
Unit_2 would not be able to statically determine where to perform the
remote call.
 
@image{xe-arch.fig}

@node Presentation Of Categorization Pragmas, Pragma Remote_Call_Interface, Architecture of A DSA Application, Tutorial on the Distributed Systems Annex
@section Categorization Pragmas

Library units can be categorized according to the role they play in a
distributed program. A categorization pragma is a library unit pragma
that restricts declarations, child units or semantic dependences of the
library unit to which it applies. There are several categorization
pragmas :

@itemize @bullet
@item Remote_Call_Interface
@item Remote_Types
@item Shared_Passive
@item Pure
@end itemize

The following paragraphs do not present the detailed semantics of these
pragmas. Their purpose is to give to the reader an intuitive overview of
what these pragmas are for. When a library unit is not categorized, this
unit is called a normal unit and cannot play a role in the distributed
application. Such a unit is duplicated on any partition in which it is
involved.

As a general remark, to avoid the development of a specific run-time
library for the DSA, the notion of remote rendez-vous has not been
introduced in Ada95. Therefore, task types and general protected types
are not allowed in the following Ada library units.

@menu
* Pragma Remote_Call_Interface::  
* Pragma Remote_Types::         
* Transmitting Dynamic Structure::  
* Pragma Shared_Passive::       
* More About Pragmas::          
@end menu

@node Pragma Remote_Call_Interface, Pragma Remote_Types, Presentation Of Categorization Pragmas, Tutorial on the Distributed Systems Annex
@section Pragma Remote_Call_Interface

@menu
* Overview of Remote Call Interface Units::  
* Regular Remote Subprograms (RCI)::  
* Remote Access To Subprograms (RAS)::  
* Remote Access To Class Wide Types (RACW)::  
* Summary on Remote Call Interface Units::  
@end menu

@node  Overview of Remote Call Interface Units, Regular Remote Subprograms (RCI), Pragma Remote_Call_Interface, Pragma Remote_Call_Interface
@subsection Overview of Pragma Remote_Call_Interface

Library units categorized with this pragma can declare subprograms to be
called and executed remotely.  This classical RPC operation is a
statically bound operation. In these units, clients and servers do not
share their memory space.

Dynamically bound calls are integrated with Ada capabilities to
dereference subprograms (remote access to subprogram - RAS) and to
dispatch on access-to-class-wide operands (remote access on class wide
types - RACW). These remote access types can be declared in a RCI
package as well.

A remote access type (RAS or RACW) can be viewed as a fat pointer or a
structure with a remote address and a local address. The remote address
describes for instance the host on which the entity has been created;
the local address describes the classical local memory address (like an
URL).

@node Regular Remote Subprograms (RCI), Remote Access To Subprograms (RAS), Overview of Remote Call Interface Units, Pragma Remote_Call_Interface
@subsection Regular Remote Subprograms (RCI)

In the following example, a RCIBank offers several remote services:
Balance, Transfert, Deposit and Withdraw. On the caller side, the bank
client will use the stub files of unit RCIBank. On the receiver side,
the bank receiver will use the skeleton files of unit RCIBank including
the body of this package.

@image{types.ads}
@image{rcibank.ads}

@node Remote Access To Subprograms (RAS), Remote Access To Class Wide Types (RACW), Regular Remote Subprograms (RCI), Pragma Remote_Call_Interface
@subsection Remote Access To Subprograms (RAS)

In the following example, several mirroring banks offer their
services. Each bank registers a reference to each of its services to a
central bank. A central bank client asks for a service of one of the
mirroring banks. In this purpose, the RCI unit RASBank defines
Balance_Type, a remote access to subprogram. An access type in a remote
unit has to be either remote access to subprogram or remote access to
class wide type.

Note that to get a remote access to subprogram the subprogram has to be
remote itself. Therefore, MirrorBank is a RCI library unit.

@image{rasbank.ads}

In the code below, a mirroring bank registers its services to the
central bank.

@image{mirrorbank.ads}
@image{mirrorbank.adb}

In the code below, a central bank client asks for a mirroring bank and
calls the Balance service of this bank by dereferencing a remote access
type.

@image{bankclient.adb}

@menu
* Remote Access To Class Wide Types (RACW)::  
@end menu

@node Remote Access To Class Wide Types (RACW), Summary on Remote Call Interface Units, Remote Access To Subprograms (RAS), Pragma Remote_Call_Interface
@subsection Remote Access To Class Wide Types (RACW)

A bank client is now connected to a bank through a terminal. The bank
wants to notify a connected client with a message on its terminal when
another client grants him with a given amount of money.

@image{terminal.ads}

In the code below, the RCI unit RACWBank defines Term_Access, a remote
access to class wide type. Term_Access becomes a reference to a
distributed object. In the next section, we will see how to derive
Term_Type, how to create a distributed object and how to use a reference
to it.

@image{racwbank.ads}

@node  Summary on Remote Call Interface Units,  , Remote Access To Class Wide Types (RACW), Pragma Remote_Call_Interface
@subsection Summary on Pragma Remote_Call_Interface

Remote call interface units:

@itemize @bullet
@item
Allow subprograms to be called and executed remotely

@item
Allow statically bound remote calls (remote subprogram)

@item
Allow dynamically bound remote calls (remote access types)

@item
Forbid variables and access types

@item
Prevent specification from depending on normal units

@end itemize

@menu
* Pragma Remote_Types::         
* Pragma Shared_Passive::       
* More About Pragmas::          
@end menu

@node Pragma Remote_Types, Pragma Shared_Passive, Pragma Remote_Call_Interface, Tutorial on the Distributed Systems Annex
@section Pragma Remote_Types

@menu
* Overview of Remote Types Units::  
* Distributed Object::          
* Transmitting Dynamic Structure::  
* Summary on Remote Types Units::  
@end menu

@node  Overview of Remote Types Units, Distributed Object, Pragma Remote_Types, Pragma Remote_Types
@subsection Overview of Pragma Remote_Types

Unlike to RCI units, library units categorized with this pragma can
define distributed objects and remote methods on them. Both RCI and RT
units can define a remote access type described above (RACW). A
subprogram defined in a RT unit is not a remote subprogram. Unlike to
RCI units, a RT unit can be duplicated on several partitions in which
case all its entities are different with each other.

@node Distributed Object, Transmitting Dynamic Structure, Overview of Remote Types Units, Pragma Remote_Types
@subsection Distributed Object

If we want to implement the notification feature proposed in the
previous section, we have to derive Term_Type. Such a derivation is
possible in a remote types unit, NewTerminal. Any object of type
New_Term_Type becomes a distributed object and any reference to such an
object becomes a fat pointer or a reference to a distributed object.

@image{newterminal.ads}

In the code below, a client registers his terminal to
RACWBank. Therefore, when any donator grants him with a given amount of
money, RACWBank is able to notify this client of the granting operation.

@image{term2client.adb}

In the code below, a second client, the donator, registers his terminal
to the bank and executes a transfer to the first client.

@image{term1client.adb}

In the code below, we describe the general design of Transfer. Classical
operations of Withdraw and Deposit are performed. Then, RACWBank
retrieves the terminal of the granted client and invoke a dispatching
operation by dereferencing a distributed object Term. The reference is
analyzed and the execution of this operation occurs on the partition to
which the distributed object belongs.

@image{racwbank.adb}

@node Transmitting Dynamic Structure, Summary on Remote Types Units, Distributed Object, Pragma Remote_Types
@subsection Transmitting Dynamic Structure

@image{stringarraystream.ads}

General access types are forbidden in public part of a remote types
unit. But this would be too restrictive. It is possible to define
private general access types as long as the user provides its
marshalling procedures. The code below describes how to transmit a
linked structure.

The code below provides an implementation of the marshalling operations
defined above:

@image{stringarraystream.adb}

@menu
* Summary on Remote Types Units::  
@end menu

@node  Summary on Remote Types Units,  , Transmitting Dynamic Structure, Pragma Remote_Types
@subsection Summary on Remote Types Units

Remote types units:

@itemize @bullet
@item
Allow to define distributed objects

@item
Allow dynamically bound remote calls (remote access type)

@item
Allow general access type (with marshalling subprograms)

@item
Allow unit duplication (like normal units)

@item
Prevent specification from depending on normal units

@end itemize

@node Pragma Shared_Passive, More About Pragmas, Pragma Remote_Types, Tutorial on the Distributed Systems Annex
@section Pragma Shared_Passive

@menu
* Overview of Shared Passive Units::  
* Summary on Shared Passive Units::  
@end menu

@node  Overview of Shared Passive Units, Summary on Shared Passive Units, Pragma Shared_Passive, Pragma Shared_Passive
@subsection Overview of Pragma Shared_Passive

The entities declared in such categorized unit library are to be mapped
on a shared address space (file, memory, database). When two partitions
use such a library unit, they can communicate by reading or writing the
same variable. This supports the shared variables paradigm. Entry-less
protected objects declared in these units provide an atomic access to
some shared data, somehow like in a transaction.

@subsection Shared and Protected Objects

In the code below, we define two kinds of shared
objects. External_Synchronization requires that the different partitions
updating this data synchronize to avoid conflicting operations on shared
objects. Internal_Synchronization provides a way to get an atomic
operation on shared objects. Note that only entry-less subprograms are
allowed in a shared passive unit.

@image{sharedobjects.ads}

@node  Summary on Shared Passive Units,  , Overview of Shared Passive Units, Pragma Shared_Passive
@subsection Summary on Pragma Shared_Passive

@itemize @bullet
@item
Allow direct access to data among different partitions

@item
Allow support on shared (distributed) memory

@item
Allow memory protection use for entryless protected objects

@item
Prevent specification from depending on normal units

@end itemize

@node More About Pragmas, Most Features in One Example, Pragma Shared_Passive, Tutorial on the Distributed Systems Annex
@section More About Categorization Pragmas

@menu
* Variables::                   
* Pragma Asynchronous::         
* Pragma All_Calls_Remote::     
* Generic Categorized Units  ::  
* Categorization Unit Dependencies::  
@end menu

@node Variables, Pragma Asynchronous, More About Pragmas, More About Pragmas
@subsection Variables, Non-Remote Access Types, RPC Failures and Exceptions

In RT or RCI units, variables are forbidden and general access types are
allowed as long as their marshaling subprograms are provided. Calls are
made at most one time. They are made exactly one time or they fail with
an exception.  Any exception raised in remote method or subprogram call
is propagated to the caller. Exceptions semantics are preserved in the
regular Ada way.

@image{internal.ads}
@image{rempkg2.ads}
@image{rempkg1.ads}

Let's say that RemPkg2, Internal and RemExcMain packages are on the same
partition Partition_1 and that RemPkg1 is on partition Partition_2.

@image{rempkg2.adb}
@image{rempkg1.adb}
@image{remexcmain.adb}

When RemPkg1.Subprogram on Partition_1 raises Internal.Exc, this
exception is propagated on Partition_2. As Internal.Exc is not defined
on Partition_2, it is not possible to catch this exception without an
exception handler @b{when others}. When we reraise this exception in
RemPkg1.Subprogram, it is propagated on Partition_1. But this time,
Internal.Exc is defined and can be handled as we would in a regular Ada
program. Of course, the exception message is also preserved.
  
@node Pragma Asynchronous, Pragma All_Calls_Remote, Variables, More About Pragmas
@subsection Pragma Asynchronous

A pragma Asynchronous allows statically and dynamically bound remote
calls to be executed asynchronously. An asynchronous procedure doesn't
wait for the completion of the remote call and lets the caller continue
its execution path. This allows a calling task to proceed without
waiting for callee procedure to complete. The procedure must have only
@b{in} parameters and any exception raised during the execution of the
remote procedure is lost. Of course, this precludes exception
propagation of remotely called procedure.

When pragma Asynchronous applies to a regular subprogram with @b{in}
parameters, any call to this subprogram will be executed
asynchronously. Declaration of AsynchronousRCI.Asynchronous gives an
example.

@image{asynchronousrci.ads}
@image{asynchronousrt.ads}

A pragma Asynchronous applies to a RAS. An asynchronous RAS can be both
asynchronous and synchronous depending on the designated subprogram. For
install, in the code above, remote call (1) is asynchronous but remote
call (2) is synchronous.

A pragma Asynchronous applies to a RACW as well. In this case, the
invocation of @b{any} method with in parameters is @b{always} performed
asynchronously. Remote method invocation (3) is asynchronous when remote
method invocation (4) is synchronous.

@image{asynchronousmain.adb}

This feature allows message passing programming. But the user has to
realize that message passing programming, and asynchronous remote calls
in particular, has several drawbacks:

@itemize

@item
Violate original (remote) procedure semantics

@item
Allow some kind of remote GOTO mechanism

@item
Prevent development and debugging in a non-distributed context

@item
Introduce potential race conditions

@end itemize

To illustrate this, let's take the following example:

@image{node2.ads}
@image{node2.adb}
@image{node1.ads}
@image{node1.adb}
@image{nondeterministic.adb}

Let's say that Main is configured on Partition_0, Node1 on Partition_1
and Node2 on Partition_2. If Node1.Send and Node2.Send procedures were
synchronous or if no latency was introduced during network
communication, we would have the following RPC order: Main remotely
calls Node1.Send which remotely calls Node2.Send which sets V to
1. Then, Main remotely calls Node2.Send and sets V to 2.

Now, let's assume that both Send procedures are asynchronous and that
the connection between Partition_1 and Partition_2 is very slow. The
following scenario can very well occur. Main remotely calls Node1.Send
and is unblocked. Immediatly after this call, Main remotely calls
Node2.Send and sets V to 2. Once this is done, the remote call to
Node1.Send completes on Partition_1 and it remotely calls Node2.Send
which sets V to 1.

@node Pragma All_Calls_Remote, Generic Categorized Units  , Pragma Asynchronous, More About Pragmas
@subsection Pragma All_Calls_Remote

A pragma All_Calls_Remote in a RCI unit can force a remote procedure
call to be routed through the communication subsystem even for a local
call. This allows to debug an application in a non-distributed
situation that is very close to the distributed one.

@node Generic Categorized Units  , Categorization Unit Dependencies, Pragma All_Calls_Remote, More About Pragmas
@subsection Generic Categorized Units  

@image{genericrci.ads}
@image{rciinstantiation.ads}
@image{normalinstantiation.ads}

Any of these categorized units can be generic. Instances of these
generic packages can be either categorized or not. In the latter, such a
unit loses its categorization property. Like any categorized unit, the
generic categorized unit can only be instantiated at the library level
and regular restrictions of categorized units apply on instantiation (in
particular on generic formal parameters).

@node Categorization Unit Dependencies,  , Generic Categorized Units  , More About Pragmas
@subsection Categorization Unit Dependencies

Each categorization pragma has very specific visibility rules. As a
general rule, RCI > RT > SP > Pure. That means that a
Remote_Types package can make visible in its specification only
Remote_Types, Shared_Passive and Pure units.

@node Most Features in One Example, A Comparison between CORBA and DSA, More About Pragmas, Tutorial on the Distributed Systems Annex
@section Most Features in One Example

The example shown on the following figure highlights most of
the features of DSA. The general system is based on a set of factories
and workers and a storage.  Each entity is a partition itself. A
factory hires a worker from a pool of workers (hire - 1) and assigns a
job (query - 2) to him. The worker performs the job and saves the
result (reply - 3) in a storage common to all the factories.  The
worker notifies the factory of the end of his job (notify - 4).

@image{full-ex.fig}
@image{storage.ads}

When a worker has achieved his job, the result should be saved in a
common storage. To do this, we define a protected area in SP package
Storage (see sample above). Two entry-less protected objects
ensure atomic access on this area.

@image{common.ads}

Types is a Remote_Types package that defines most of the remote services
of the above system (see sample above). First, we define a way for the
workers to notify the end of his job.  This callback mechanism is
implemented using RAS Notify.

@image{newworkers.ads}

We define an abstract tagged type Worker which is intended to be the
root type of the whole distributed objects hierarchy. Assign allows a
factory to propose to a worker a job and a way to notify its employer
the end of this job. Any_Worker is a remote access to class wide type
(RACW). In other words, it is a reference to a distributed object of any
derived type from Worker class.

@image{newnewworkers.ads}

NewWorker is derived from type Worker and Assign is overridden. Sample
above shows how to derive a second generation of workers NewNewWorker
from the first generation NewWorker. As mentioned above, this RT package
can be duplicate on several partitions to produce several types of
workers and also several remote workers.

@image{workercity.ads}

In sample above, we define an unique place where workers wait for
jobs. Workers is a Remote_Call_Interface package with services to hire
and free workers. Unlike to Remote_Types packages, Remote_Call_Interface
packages cannot be duplicated.

@image{workercity.ads}
@image{newfactory.ads}

In order to use even more DSA features, Factory is defined as a generic
RCI package (see sample above). Any instantiation defines a new factory
(see sample above). To be RCI, this instantiation has to be categorized
once again.

@node A Comparison between CORBA and DSA,  , Most Features in One Example, Tutorial on the Distributed Systems Annex
@section A Comparison between CORBA and DSA

@menu
* CORBA Architecture::          
* Interface Definition Language::  
* Network Communication Subsystem::  
* Distributed Application Development::  
* Some elements of comparison::  
@end menu

@node  CORBA Architecture, Interface Definition Language, A Comparison between CORBA and DSA, A Comparison between CORBA and DSA
@subsection CORBA Architecture

CORBA is an industry-sponsored effort to standardize the distributed
object paradigm via the CORBA Interface Definition Language (IDL).  The
use of IDL makes CORBA more self-describing than any other client/server
middleware. The Common Object Request Broker: Architecture and
Specification, revision 2.2 describes the main features of CORBA which
are Interface Definition Language, Language Mappings, Stubs, Skeletons
and Object Adapters, ORB, Interface Repository, Dynamic Invocation, ORB
protocols and CORBA services.

@image{corba-arch.fig}

The IDL specifies modules, constants, types and interfaces. An object
interface defines the operations, exceptions and public attributes a
client can invoke or access. CORBA offers a model based only on
distributed objects. In some respects, it can be compared to Java as
this language provides only an object-oriented programming model, and
discards the classical structured programming model.

An IDL pre-compiler generates client stubs and server skeletons in a
host language (C++, C, Java, Smalltalk, Ada95); a language mapping
specifies how CORBA constructs are implemented in the host language. Of
course, if the host language is Java or C++, as the IDL is already
closed to these languages, the mapping is quite straightforward. If the
language is Ada95, then the user can have significantly much work to
suit the generated code to his actual needs.

The IDL pre-compiler produces two host language source files: a client
file called stub and a server file called skeleton. These files are
specific to a vendor and product, as they make calls to a proprietary
communication subsystem, but their structure is supposed to follow a
standard canvas.  The client stubs convert user queries into requests to
the ORB, which transmits these requests through an object adaptor to the
server skeleton.

@node Interface Definition Language, Network Communication Subsystem, CORBA Architecture, A Comparison between CORBA and DSA
@subsection Interface Definition Language

In DSA, the IDL is a subset of Ada95. The user identifies at compile
time interface packages. Some interface packages are categorized using
pragmas and these interface packages have to be unit libraries. There
are three kinds of categorization pragmas:

In CORBA, the IDL is a descriptive language; it supports @t{C++}
syntax for constant, type and operation declarations. From IDL
descriptions, a pre-compiler can directly generate client header files
and server implementation skeletons.

An IDL file can start by defining a module. This provides a name-space
to gather a set of interfaces. This is a way to introduce a level of
hierarchy (<@i{module}>::<@i{interface}>::<@i{operation}>). The Ada95
binding maps this element into a (child) package. @t{\#include}
will make any other namespaces visible.

A module can define interfaces. An interface defines a set of
methods that a client can invoke on an object. An interface can also
define exceptions and attributes. An exception is like a @t{C++}
exception and a component can be attached to it. An attribute is a
component field. For each of them, the implementation automatically
creates Get and Set operations. Only Get is provided for
readonly attributes. An interface can derived from one or more
interfaces (multiple inheritance).

The Ada95 binding maps this element into a package or a child
package. For the client stub, the implementation will automatically
create a tagged type named Ref (which is derived from CORBA.Object.Ref
or from the derived type Ref of another interface) in a package named
like the interface. For the server skeleton, the implementation will
automatically create a tagged type named Object (which is derived from
an implementation defined private tagged type Object) in a package named
Impl, child package of a package named like the interface
(<@i{interface}>.Impl).

@image{naming.idl}

A method is defined by its unique name (no overloading allowed) and its
signature (the types of its formal parameters). Each parameter can be of
mode @b{in}, @b{out} or @b{inout}, whose meaning is
comparable to their Ada homonyms. Every exception that can be raised by
a method must also be declared as part of the method signature.

In addition, the @b{oneway} attribute can be applied to a subprogram,
making it an asynchronous method. The caller of an asynchronous method
will continue its execution path without waiting for the completion of
the remote call.

Most CORBA data types map naturally onto predefined Ada types, with
the exception of @t{any} and @t{sequence}. @t{any},
that can designate any CORBA type, will be mapped onto a stream type
with @t{read} and @t{write} operations. A @t{sequence}
holds a list of data with a given type and will be represented in Ada
using a pair of lengthy generic packages (the declaration itself would
take about 20 pages). One may note that the CORBA @t{string} type
is mapped onto the @t{Unbounded\_String} predefined Ada type.

The Ada95 mapping provides special mechanisms to implement two difficult
CORBA features. First, it provides a way to implement multiple
inheritance. As described above, an Ada95 package defines a type derived
from the first interface and extends the list of its primitives to
achieve inheritance from other interfaces. Another difficult feature of
CORBA comes from forward declarations. In Ada, two package
specifications cannot ``with'' each others, but this can occur between
to IDL interfaces (see IDL specification above). To solve this, the
mapping proposes to create ``forward'' packages. This can result in a
very non-intuitive situation where the client stub does not ``with'' its
usual interface packages but ``forward'' packages.

We can note that to develop a distributed application with CORBA, a good
understanding of the IDL is not enough. The user has also to understand
very well the language mapping defined for the host language he wants to
use. This can introduce a high level of complexity in the user code.

Interface information coded in IDL can be stored in an on-line database
of object definitions called Interface Repository. A CORBA specification
describes how the interface repository is organized and how to retrieve
information on IDL from this repository. The reader familiar with Ada95
tools will note that this information is very close to what ASIS would
provide.

This interface repository becomes useful when a client wants to invoke a
method for which he does not know the signature. CORBA defines an API to
retrieve in the database the data that defines the server operation. It
also defines a way to build a full request by generating the parameters
needed for a method invocation.

Basically, this API allows the client to explore the repository classes
to obtain a module definition tree. From this tree, the client is able
to extract subtrees defining constants, types, exceptions and
interfaces. From an interface subtree, the client can select an
operation with its list of parameters (type, name and mode) and
exceptions.

Therefore, the client has three ways to invoke the request. He can send
the request and wait for the results. But he can also defer to get the
result or ask to ignore the results (oneway). This mechanism is known as
Dynamic Invocation Interface (DII). This mechanism is also available on
the server side and is known as Dynamic Skeleton Interface. Of course,
these mechanisms are powerful but very complex and tedious to use. Most
of the users will keep working with static invocations.

@node Network Communication Subsystem, Distributed Application Development, Interface Definition Language, A Comparison between CORBA and DSA
@subsection Network Communication Subsystem

@menu
* DSA PCS::                     
* CORBA ORB::                   
@end menu

@node DSA PCS, CORBA ORB, Network Communication Subsystem, Network Communication Subsystem
@subsubsection DSA PCS

In the DSA world, everything that is not done by the compiler in regard
to the distribution belongs to the partition communication subsystem
(PCS). For example figuring out on which partition a package that will
be called remotely is located is part of the PCS responsibility.

The PCS entry points are well defined in the Distributed Systems Annex
and described in the @t{System.RPC} package declaration.  By looking at
this package, one can notice that there is nothing related to abortion
of remote subprogram calls, although the Annex states that if such a
call is aborted, an abortion message must be sent to the remote
partition to cancel remote processing. That means that the PCS is in
charge of detecting that a call to one of its entry points has been
aborted and must send such an abortion message, without any help from
the compiler.

Another interesting characteristic of the PCS is its behavior regarding
unknown exceptions. When an exception is raised as the result of the
execution of a remote subprogram call, it is propagated back to the
caller. However, the caller may not have any visibility over the
exception declaration, but may still catch it with a @t{when
  others} clause. But if the caller does not catch it and let it be
propagated upstream (maybe in another partition), and if the upstream
caller has visibility over this exception, it must be able to catch it
using its name. That means that the PCS must recognize that a
previously unknown exception maps onto a locally known one, for
example by being able to dynamically register a new exception into the 
runtime.

@node CORBA ORB,  , DSA PCS, Network Communication Subsystem
@subsubsection CORBA ORB

CORBA has adopted a much more fragmented point of view in regard to
services: as much services as possible will be defined externally.  For
example, the naming service (whose duty is to locate remote objects from
their name) is itself a distributed object with a standardized IDL
interface.

While this looks like more pure an approach, this may also induce some
slowdowns. Being itself a distributed object, the naming service cannot
be optimized specifically for the ORB needs.  Also some magic is
required in the ORB to be able to locate the naming service itself
(chicken and egg problem).

Regarding exception propagation, an ORB is not able to propagate an
exception that has not been declared in the IDL interface. This
restriction, although annoying because it restricts the usage of
exceptions, is understandable given the multi-language CORBA approach:
what should be done for example with a C++ exception which reaches a
caller written in Ada?

@node Distributed Application Development, Some elements of comparison, Network Communication Subsystem, A Comparison between CORBA and DSA
@subsection Distributed Application Development

@menu
* DSA Application Development::  
* CORBA Application Development::  
@end menu

@node DSA Application Development, CORBA Application Development, Distributed Application Development, Distributed Application Development
@subsubsection DSA Application Development

The DSA does not describe how a distributed application should be
configured. It is up to the user (using a partitioning tool whose
specification is outside the scope of the annex) to define what the
partitions in her program are and on which machines they should be
executed.

GLADE provides a Configuration Tool and a Partition Communication
Subsystem to build a distributed application. The GNATDIST tool and its
configuration language have been purposely designed to let the user
partition her program and specify the machines where the individual
partitions will be executing. The Generic Ada Reusable Library for
Interpartition Communication (GARLIC) is a high level communication
library that implements with object-oriented techniques the interface
between the Partition Communication Subsystem defined in the Reference
Manual and the network communication layer.

@node CORBA Application Development,  , DSA Application Development, Distributed Application Development
@subsubsection CORBA Application Development

@c \reviewers{We shall give more details on the distributed
@c   application development in the final paper. This will also describe
@c   the services available in CORBA: CosNaming, CosEvents,
@c   ...}

@node Some elements of comparison,  , Distributed Application Development, A Comparison between CORBA and DSA
@subsection Some elements of comparison

CORBA provides an outstanding and very popular framework. The IDL syntax
is close to @t{C++}. The object model is close to @t{Java}: CORBA
defines only distributed objects. Furthermore, the stub and skeleton
generated code is close to Java with two root classes, Ref for clients
and Object for servers.

DSA provides a more general model. This includes distributed objects,
but also regular remote subprograms and references on remote
subprograms. Shared passive packages can be defined as an abstraction
for a (distributed) shared memory, a persistency support or a database.
Basically, the IDL is a subset of Ada95 and the remote services are
defined in packages categorized by three kinds of pragmas (RCI, RT,
SP). The distributed boundaries are more transparent as the application
is not split into IDL sources and others sources in some host languages.

To use a client stub or server skeleton, a CORBA user has to perfectly
understand the Ada95 mapping or any other language mapping. Some types
can be quite difficult or expensive to map like exceptions, any, string
and sequence. Some IDL features are also difficult to implement in
Ada95 like multiple inheritance and forward declarations.

In DSA, any Ada type can be used except access types, but this can be
solved by providing the marshaling operations for such a type. The
exception model is entirely preserved. Overloading is allowed in DSA
(not in CORBA). The user can also define generic packages and use mixin
mechanism to obtain some kind of multiple inheritance.

The DSA user can design, implement and test his application in a
non-distributed environment, and then switch to a distributed situation.
With this two-phase design approach, the user always works within his
favorite Ada95 environment. Pragma All_Calls_Remote also facilitates
debugging of a distributed application in a non-distributed context.

The CORBA user has to re-adapt his code to the code generated by the
pre-compiler from the IDL file anytime it is modified. He also has to
use the predefined CORBA types instead of Ada standard types; he has to
call adaptor services to obtain remote object references or a specific
naming service.

As Ada95 is the IDL, the user will not have to deal with any generated
stub or skeleton code. The configuration environment will take care of
updating object, stub and skeleton files when sources have been
updated. The system will automatically take in charge some naming
services like declaring RCI services. It will also take care of aborting
remote procedure calls, detecting the distributed termination, checking
version consistency between clients and servers or preserving and
propagating remote exceptions.

CORBA is a very rich but very complex system. The drawbacks include the
high learning curve for developing and managing CORBA applications
effectively, performance limitations, as well as the lack of portability
and security.  These drawbacks are the price to pay for language
interoperability, a facility the Ada95-oriented DSA does not provide.

Using its IDL, the OMG has described a number of @i{Common Object
Services} that are frequently needed to develop distributed
systems. Unfortunately, these specifications are limited to an IDL
description, and most of the semantics is up to the vendor. The DSA
misses such a user-level library including basic distributed software
components.  More generally, the lack of component libraries has always
been a problem for Ada.

@node Getting Started With GLADE, Index, Tutorial on the Distributed Systems Annex, Top
@chapter Getting Started With GLADE

This chapter describes the usual ways of using GLADE to compile Ada
distributed programs.

@menu
* Introduction To GLADE::       
* How to Use Gnatdist to Configure a Distributed Application::  
* Gnatdist Command Line Options::  
* Gnatdist Behind the Scenes::  
* Tracing Facilities::          
* Remote Shell Notes::          
* Warnings::                    
* Filtering::                   
* Trace/Replay Debugging::      
* Restrictions::                
@end menu

@node Introduction To GLADE, How to Use Gnatdist to Configure a Distributed Application, Getting Started With GLADE, Getting Started With GLADE
@section Introduction To GLADE

An Ada 95 distributed application comprises a number of partitions
which can be executed concurrently on the same machine or, and this is
the interesting part, can be distributed on a network of machines.
The way in which partitions communicate is described in Annex E of the
Ada 95 reference manual.

A partition is a set of compilation units which are linked together to
produce an executable binary. A distributed program comprises two or
more communicating partitions.

The distributed systems annex does not describe how a distributed
application should be configured. It is up to the user to define what
are the partitions in his program and on which machines they should be
executed.

The tool gnatdist and its configuration language have been purposely
designed to allow you to partition your program and specify the
machines where the individual partitions are to execute on.

gnatdist reads a configuration file (whose syntax is described below)
and builds several executables, one for each partition. It also takes
care of launching the different partitions (default) and to pass
arguments specific to each partition.

@node How to Use Gnatdist to Configure a Distributed Application, Gnatdist Command Line Options, Introduction To GLADE, Getting Started With GLADE
@section How to Use Gnatdist to Configure a Distributed Application

@itemize

@item
Write a non-distributed Ada application. Use the categorization pragmas
to specify the packages that can be called remotely. The Shared_Passive
categorization pragma is not yet implemented. The Remote_Call_Interface
and Remote_Types categorization pragmas are.

@item
When this non-distributed application is working, write a configuration
file that maps your categorized packages onto partitions. Don't forget
to specify the main procedure of your distributed application (see
below).

@item
Type `gnatdist configuration-file'.

@item
Start your distributed application by invoking the start-up shell script
or Ada program (depending on the "pragma Starter" option, see below).

@end itemize

@node Gnatdist Command Line Options, Gnatdist Behind the Scenes, How to Use Gnatdist to Configure a Distributed Application, Getting Started With GLADE
@section Gnatdist Command Line Options

@smallexample
gnatdist [switches] configuration-file [list-of-partitions]
@end smallexample

The switches of gnatdist are, for the time being, exactly the same as
for gnatmake.  Read the gnatinfo.txt file from the GNAT distribution for
info on these switches. By default gnatdist outputs a configuration
report and the actions performed. The switch -n allows gnatdist to skip
the first stage of recompilation of the non-distributed application.

All configuration files should end with the `.cfg' suffix. There may be
several configuration files for the same distributed application, as you
may want to use different distributed configurations according to your
computing environment.

If a list  of  partitions is provided  on the  command line,  only these
partitions will be  build. In the  following  configuration example, you
can type : gnatdist configuration partition_2 partition_3.

@node Gnatdist Behind the Scenes, Tracing Facilities, Gnatdist Command Line Options, Getting Started With GLADE
@section Gnatdist Behind the Scenes

Here is what goes on behind the scenes in gnatdist when building a
distributed application:

@itemize
@item
Each compilation unit in the program is compiled into an object module
(as in non distributed applications). This is achieved by calling
gnatmake on the sources of the various partitions.  This step can be
skipped by using the -n option.

@item
Stubs are compiled into object modules (a stub is the software that
allows a partition running on machine A to communicate with a partition
running on machine B).  Several timestamp checks are performed to avoid
useless recompilation.

@item
gnatdist performs a number of consistency checks, for instance it
checks that all packages marked as remote call interfaces (RCI, see
LRM annex E) are mapped onto partitions. It also checks that an
RCI package is mapped onto only one partition.

@item
Finally, the executables for each partition in the program are
created. The code to launch partitions is embedded in the main
partition except if another option has been specified (pragma
Starter). In this case, a shell script (or nothing) is generated to
start the partitions on the appropriate machines. This is specially
useful when one wants to write client / server applications where
the number of instances of the partition is unknown.

@end itemize

@section The Configuration Language

The configuration language is "Ada-like". Because of its simplicity,
it is described by means of an example. As the capabilities of GLADE
will evolve, so will this configuration language.

Every keyword and construct defined in the configuration language have
been used in the following sample configuration file.

@newpage

@smallexample
---------------------------
--  File `my_config.cfg' --
---------------------------

--  Typically after having created the following configuration file
--  you would type
--
--      gnatdist my_config.cfg
--
--  If you wish to build only certain partitions then list the partitions
--  to build on the gnatdist command line as follows:
--
--      gnatdist my_config.cfg   partition_2 partition_3

configuration My_Config is
  --  The name of the file prefix must be the same as the name of
  --  the configuration unit, in this example `my_config'. The file
  --  suffix must be `cfg'. For a given distributed application
  --  you can have as many configuration files as you wish.

  Partition_1 : Partition := ();
  procedure Master_Procedure is in Partition_1;
  --  Partition 1 contains no RCI package.
  --  However, it will contain the main procedure of the distributed
  --  application, called `Master_Procedure' in this example. If the
  --  line `procedure Master_Procedure is in Partition_1;' was missing
  --  Partition 1 would be completely empty. This is forbidden, a
  --  partition has to contain at least one library unit.
  --  
  --  gnatdist produces an executable with the name of Master_Procedure
  --  which will starts the various partitions on their host machines
  --  in the background.  The main partition is launched in the foreground.
  --  Note that by killing this main procedure the whole distributed
  --  application is halted. 

  Partition_2, Partition_3 : Partition;

  for Partition_2'Host use "foo.bar.com";
  --  Specify the host on which to run partition 2.

  function Best_Node (Partition_Name : String) return String;
  pragma Import (Shell, Best_Node, "best-node");
  for Partition_3'Host use Best_Node;
  --  Use the value returned by an a program to figure out at execution time
  --  the name of the host on which partition 3 should execute.
  --  For instance, execute the shell script `best-node' which takes
  --  the partition name as parameter and returns a string giving the name of
  --  the machine on which partition_3 should be launched.

  Partition_4 : Partition := (RCI_B5);
  --  Partition 4 contains one RCI package RCI_B5
  --  No host is specified for this partition. The startup script
  --  will ask for it interactively when it is executed.

  for Partition_1'Storage_Dir use "/usr/you/test/bin";
  --  Specify the directory in which the executables in each partition
  --  will be stored.

  for Partition'Storage_Dir use "bin";
  --  Specify the directory in which all the partition executables
  --  will be stored. Default is the current directory.

  procedure Another_Main;
  for Partition_3'Main use Another_Main;
  --  Specify the partition main subprogram to use in a given
  --  partition.

  for Partition_3'Reconnection use Blocked_Until_Restart;
  --  Specify a reconnection policy on Partition_3 crash. Any attempt
  --  to reconnect to Partition_3 when this partition is dead will
  --  be kept blocking until Partition_3 restart. As a default, any
  --  restart is rejected (Rejected_ON_Restart). Another policy is to
  --  raise Communication_Error on any reconnection attempt until
  --  Partition_3 has been restarted.

  for Partition_4'Command_Line use "-v";
  --  Specify additional arguments to pass on the command line when a 
  --  given partition is launched.

  for Partition_4'Termination use Local_Termination;
  --  Specify a termination mechanism for partition_4. The default is
  --  to compute a global distributed termination. When Local_Termination
  --  is specified then a partition terminates when the local termination
  --  is detected (standard ada termination).

  pragma Starter (Method => Ada);
  --  Specify the kind of startup method you would like. There are 3
  --  possibilities: Shell, Ada and None. Specifying `Shell' builds a shell
  --  script. All the partitions will be launched from a shell script.
  --  If `Ada' is chosen, then the main Ada procedure itself is used to launch
  --  the various partitions. If method `None' is chosen, then
  --  no launch method is used and you have to start each partition
  --  manually. 
  --
  --  If no starter is given, then an Ada starter will be used.
  --
  --  In this example, Partition_2, Partitions_3 and Partition_4 will be
  --  started from Partition_1 (ie from the Ada procedure Master_Procedure).

  pragma Boot_Server
    (Protocol_Name => "tcp", Protocol_Data => "`hostname`:`unused-port`");
  --  Specify the use of a particular boot server. It is especially
  --  useful when the default port 5555 used by GARLIC is already assigned.

  pragma Version (False);
  --  It is a bounded error to elaborate a partition of a distributed
  --  program that contains a compilation unit that depends on a
  --  different version of the declaration of RCI library unit than
  --  that included in the partition to which the RCI library unit was
  --  assigned. When the pragma Version is set to False, no
  --  consistency check is performed.

  Channel_1 : Channel := (Partition_1, Partition_4);
  Channel_2 : Channel := (Partition_2, Partition_3);
  --  Declare two channels. Other channels between partitions remain unknown.

  for Channel_1'Filter use "ZIP";
  --  Use transparent compression/decompression for the arguments and results
  --  of any remote calls on channel "Channel_1", i.e. between "Partition_1"
  --  and "Partition_4".

  for Channel_2'Filter use "My_Own_Filter";
  --  Use filter "My_Own_Filter" on "Channel_2". This filter must be imple-
  --  mented in a package "System.Garlic.Filters.My_Own_Filter".

  for Partition'Filter use "ZIP";
  --  For all data exchanged between partitions, use the filter "ZIP". (I.e.
  --  for both arriving remote calls as well as for calls made by a parti-
  --  tion.)

  pragma Registration_Filter ("Some_Filter");
  --  "Some_Filter" will be used to exchange a filter's parameters between
  --  two partitions. "Some_Filter" itself must be an algorithm that doesn't
  --  need its own parameters to be filtered again!

  --  On all other channels (i.e., for remote calls between partitions where
  --  no channel was declared), filtering is not used.

begin
  --  The configuration body is optional. You may have fully described your
  --  configuration in the declaration part.

  Partition_2 := (RCI_B2, RCI_B4, Normal);
  --  Partition 2 contains two RCI packages RCI_B2 and RCI_B4
  --  and a normal package. A normal package is not categorized.

  Partition_3 := (RCI_B3);
  --  Partition 3 contains one RCI package RCI_B3

end My_Config;

@end smallexample

@node Tracing Facilities, Remote Shell Notes, Gnatdist Behind the Scenes, Getting Started With GLADE
@section Tracing Facilities

To trace your application, you can set two environment variables to
true. The variable S_RPC will provide info on what is going on the
execution of remote procedure calls (resolved in System.RPC -
s-rpc.adb). The variable S_PARINT will provide info on partitions and
units status (resolved in System.Partition_Interface -
s-parint.adb). For instance, using sh, bash or zsh, type:

@smallexample
S_RPC=true;    export S_RPC
S_PARINT=true; export S_PARINT
@end smallexample

@node Remote Shell Notes, Warnings, Tracing Facilities, Getting Started With GLADE
@section Remote Shell Notes

To start a partition, the main partition executable executes a remote
shell. Thus you have to make sure that you are authorized to execute a
remote shell on the remote machine. In this case, a first step would
be to add into your ${HOME}/.rhosts file a line like :
<remote-machine> <your-username>

If you are not authorized at all, you can bypass this problem. All you
have to do is:

@itemize

@item
Open a session on each machine listed in your configuration file.

@item
If MAIN_PART is the partition that includes the main procedure and if
you want to start MAIN_PART on host MAIN_HOST:

@itemize

@item
Choose a TCP port number PORT_NUM (gnatdist default is 5555 when using a
shell starter, randomly chosen when using an Ada starter).

@item
Then for each partition PART, start manually the corresponding
executable on the corresponding host as follows

@smallexample
% PART [--nolaunch] [--slave] --boot_server tcp://MAIN_HOST:PORT_NUM
@end smallexample

The --nolaunch parameter must be included for the main partition, it
means that this partition is not in charge of launching others. The
--slave parameter must be included for other partitions, meaning that in
no case the name server is located on them.

@end itemize

@item
If you want to kill the distributed application before it terminates,
kill MAIN_PART.

@end itemize

@node Warnings, Filtering, Remote Shell Notes, Getting Started With GLADE
@section Warnings

All GLADE intermediate files (object files, etc) are stored under a
common directory named "dsa". You may remove this whole directory and
its content when you do not intend to rebuild your distributed
applications.


@node Filtering, Trace/Replay Debugging, Warnings, Getting Started With GLADE
@section Filtering

GLADE contains a transparent extensible filtering mechanism allowing the
user to define various data transformations to be performed on the arguments
and return values of remote calls. One possible application would be to
compress all data before sending it and to decompress it on the receiving
partition.

With GLADE, it is no longer necessary that the application take care of such
transformations. Instead, users can write their own data transformations and
hook them into GLADE so that they are automatically and transparently applied
depending on the configuration of the distributed application.

@menu
* Configuring filtering::       
* Implementing Your Own Filters::  
@end menu

@node Configuring filtering, Implementing Your Own Filters, Filtering, Filtering
@subsection Configuring filtering

As a default, no filtering is performed by GLADE. As a default, the
compression filter is available. Therefore, you can configure your
distributed application in order to use this filter.

The configuration language not only knows about partitions, it also
knows about the connections between them. Such a connection is called a
"Channel" and represents a bi-directional link between two
partitions. In order to define filtering, one must first declare the
channels between the partitions of an application:

@smallexample
A_Channel : Channel := (Partition_1, Partition_2);
@end smallexample

This gives the link between partitions "Partition_1" and "Partition_2"
the name "A_Channel". It is not possible to declare more than one
channel between the same two partitions.

Now that this channel is known, the data transformation that is to be
applied on all data sent through it can be defined:

@smallexample
for A_Channel'Filter use "ZIP";
@end smallexample

This specifies that all data sent over this channel should be
transformed by the filter named "ZIP". (There must be a filter with this
name, implemented in the package 'System.Garlic.Filters.Zip'.)

Some filtering algorithms require that some parameters must be sent to the
receiver first to enable it to correctly de-filter the data. If this is
the case, it may be necessary to filter these parameters again. For such
purposes, it is possible to install a global filter for all partitions,
which then will be used to filter the parameters of other filters. This
filter is called the "registration filter". It can be set by a pragma:

@smallexample
pragma Registration_Filter ("Filter_Name");
@end smallexample

It may also be useful to specify that a partition use a certain filter
for all remote calls, regardless of the channel (i.e., regardless of the
partition that'll receive the remote call). This can be specified using
the attribute 'Filter on a partition:

@smallexample
for Partition_1'Filter use "ZIP";
@end smallexample

or even

@smallexample
for Partition'Filter use "ZIP";
@end smallexample

(The latter set the default filter for all partitions of the application,
the former only sets the default filter for the partition "Partition_1".)
It is also possible to apply a default filter and to override this default
for specific channels:

@smallexample
My_Channel : Channel := (Partition_1, Partition_2);

for My_Channel'Filter  use "ZIP";
for Partition_1'Filter use "Some_Other_Filter";
@end smallexample

This makes 'Partition_1' use "Some_Other_Filter" for all remote calls
except for any communication with 'Partition_2', where the filter "ZIP"
is applied.

Gnatdist takes care of consistency checking of a filter definition. By
default, no filtering is done. Filtering is only active if specified
explicitly in the configuration file.
 
@node Implementing Your Own Filters,  , Configuring filtering, Filtering
@subsection Implementing Your Own Filters

As has been briefly mentioned above, a filter with a name "NAME" must be
implemented in a package called 'System.Garlic.Filters.Name'. You may
write your own filters, which must implement their filtering of data in
the primitive operations of a type derived from the type
'System.Garlic.Filters.Filter_Type'. Your filter package must then
register an instance of your newly derived type with GLADE by calling
'System.Garlic.Filters.Register'. From that on, your filter is ready to
be used.

For more information on how to write your own filter packages see the
sample implementation of a ZIP filter in files 's-gafizi.ad[bs]' in the
distribution. You might also want to look at the example in the
'Filtering' directory of the GLADE distribution.


@node Trace/Replay Debugging, Restrictions, Filtering, Getting Started With GLADE
@section Trace/Replay Debugging

GLADE has a facility for trace/replay based debugging. If trace mode
is turned on, GLADE will record all messages received by a partition
into a trace file. The trace file can then be used to replay the
execution of the partition, in isolation.

To get a partition to generate a trace file, it has to be passed the
command line argument "--trace". This is most easily done by using the
"for Partition'Command_Line use..." construct (described above) in the
configuration file to add "--trace" to the command lines of the
partitions whose executions are to be replayed. When the application
has been built, starting it using the starter, as usual, will then
result in the trace files being generated.

By default, the file name of the trace file is the name of the
partition's executable (i.e. the string returned by the standard
procedure Ada.Command_Line.Command_Name) with a trailing
".trace". This can be changed with the "--trace_file othername"
command line argument. Note that since the remote partitions are
launched with rsh under Unix, the current directory during execution
will be the user's home directory. This is no problem when using the
default trace file name, because the executable's name will include
the absolute path. When using the "--trace_file" option, on the other
hand, if you don't want the trace file to be created/read in the home
directory, the absolute path will have to be included in the desired
name.

In order to replay a partition whose execution has been previously
traced, the command line argument "--replay" is required. In addition,
the special boot server location "replay://" has to be specified,
i.e. by using the "--boot_server replay://" command line argument.

Example: To replay a traced execution of partition whose executable is
named PART, you would start it with the command

@smallexample
% PART [--nolaunch] [--slave] --replay --boot_server replay://
@end smallexampl

possibly under the control of a debugger, such as gdb.

Since the exact contents of the messages received is recorded,
differences in input from external sources (such as standard input)
during replay will most likely give unexpected results. Also, replay
of applications whose behavior is inherently non-deterministic will be
problematic.

N.B. It is important that the same executable is used for replay as
when the trace file was generated, otherwise strange behavior can be
expected.


@node Restrictions,  , Trace/Replay Debugging, Getting Started With GLADE
@section Restrictions

Static remote procedures, asynchronous remote procedures, remote
access to class wide types, remote access to subprogram and
asynchronous transfer of control with remote procedures are
implemented. Remote types packages are implemented.

Pragma All_Calls_Remote has been implemented.

Shared passive packages are unimplemented.

Language-defined exceptions propagate well through different
partitions.


@node Index,  , Getting Started With GLADE, Top
@unnumbered Index

@printindex cp

@contents

@bye
