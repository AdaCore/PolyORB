\input texinfo @c -*-texinfo-*-
@setfilename glade_ug.info
@settitle GLADE User's Guide
@setchapternewpage odd
@syncodeindex fn cp

@titlepage

@title GLADE User's Guide
@subtitle GLADE, GNAT Library for Ada Distributed Environment
@subtitle Version 2.01 (DRAFT)
@subtitle December 16, 1997
@author Laurent Pautet, Samuel Tardieu

@page
@vskip 0pt plus 1filll

@copyright{} Copyright 1997-1997, Free Soft Foundation.

GLADE is free software; you can redistribute it and/or modify it under
terms of the GNU General Public License as published by the Free
Software Foundation; either version 2, or (at your option) any later
version. GNAT is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANT
ABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public
License for more details.  You should have received a copy of the GNU
General Public License along with GNAT; see file COPYING. If not, write
to the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.

@end titlepage

@ifinfo
@node Top, Tutorial on the Distributed Systems Annex, (dir), (dir)
@top GLADE User's Guide

GLADE is the GNAT implementation of Distributed Systems Annex.

@menu
* Tutorial on the Distributed Systems Annex::  
* Getting Started With GLADE::  
* Index::                       
@end menu

@end ifinfo

@node Tutorial on the Distributed Systems Annex, Getting Started With GLADE, Top, Top
@chapter Tutorial on the Distributed Systems Annex

@menu
* Introduction To Distributed Systems::  
* The Distributed System Annex::  
* Architecture of A DSA Application::  
* Presentation Of Categorization Pragmas::  
* Pragma Remote_Call_Interface::  
* Pragma Remote_Types::         
* Pragma Shared_Passive::       
* More About Pragmas::          
* Most Features in One Example::  
* A Comparison between CORBA and DSA::  
@end menu

@node Introduction To Distributed Systems, The Distributed System Annex, Tutorial on the Distributed Systems Annex, Tutorial on the Distributed Systems Annex
@section Introduction To Distributed Systems

@menu
* Using OS Network Services::   
* Using A Middleware Environment::  
* Using A Distributed Language::  
@end menu

A distributed system architecture comprises a network of computers and the
software components that execute on the computers. Such architectures are
commonly used to improve the performance, reliability, and reusability of
complex applications. Typically, when there is no shared address space
available to remotely-located components, components must communicate using
some form of message-passing abstraction.

@node Using OS Network Services, Using A Middleware Environment, Introduction To Distributed Systems, Introduction To Distributed Systems
@subsection Using OS Network Services

There are several programming techniques for developing distributed
applications. These applications have traditionally been developed using
network programming interfaces such as sockets. Programmers explicitly
have to perform calls to operating system services, a task that can be
tedious and error-prone. This includes initializing socket connection
and determining peer location, marshaling and unmarshaling data
structures, sending and receiving messages, debugging and testing
several programs at the same time and porting on several platforms to
hide subtle differences between various network interfaces.

Of course, this code can be encapsulated in wrappers to reduce its
complexity but it is clear that most of it can be automatically
generated. Message passing diverts developer's attention from the
application domain. The query and reply scenario is a classical scheme
in distributed applications; using message passing in such a situation
could be compared to using a ``goto'' mechanism in a non-distributed
application.  This is known to be a significant problem in the domain of
modern programming languages. A more robust design would be to use a
structured approach based on procedure call.

In some respects, this network programming issue can be compared to the
multi-threading programming issue. An user can decide to split his code
in several pieces and to multiplex the thread executions himself using a
table-driven model. The scheduling code would be embedded into the user
code. This solution is error-prone and fragile in regard to any future
modification.

@node Using A Middleware Environment, Using A Distributed Language, Using OS Network Services, Introduction To Distributed Systems
@subsection Using A Middleware Environment

A middleware environment is intended to provide high level abstractions
in order to easily develop user applications.  Environments like CORBA
or Distributed Computing Environment (DCE) propose an approach to
develop client/server applications using the Remote Procedure Call model
(RPC). The RPC model is inspired from the query and reply
scheme. Compared to a regular procedure call, arguments are pushed into
a stream along with some data pointing out which remote procedure is to
be used and the stream is transmitted over the network to the
server. The server decodes the stream, does the regular subprogram call,
then put the output parameters into another stream along with the
exception (if any) raised by the subprogram and sends this stream back
to the caller. The caller decodes the stream and raises the exception if
needed.

CORBA provides the same enhancements to the remote procedure model that
object languages provide to classical procedural languages.  This also
includes encapsulation, inheritance, type checking, and
exceptions. These features are offered through an Interface Definition
Language (IDL).

The middleware communication framework provides all the machinery to
perform, somewhat transparently, remote procedure calls or remote object
method invocations. For instance, each CORBA interface communicates
through an Object Request Broker (ORB). A communication subsystem such
as an ORB is intended to allow applications to use objects without being
aware of their underlying message passing implementation. But the user
may also require a large number of complex services to develop the
distributed application. Some of them are definitively needed like a
location service that allows clients to reference remote services via
higher level names instead of a traditional scheme for addressing remote
services involving Internet host addresses and communication port
number. Other services provide domain independent interfaces that are
frequently used by distributed applications like naming services.

If we get back to the multi-thread programming comparison, the
middleware solution is close to what a POSIX library or a language like
Esterel would propose to develop a concurrent application. A middleware
framework like DCE is close to a POSIX library in terms of abstraction
levels. Functionalities are very low-level and very complex. CORBA is
closer to Esterel in terms of development process.  The control part of
the application can be specified in a description language. The
developer then has to fill in automatically generated source code (stub
and skeletons) to call the computation part of the application. The
distribution is a pre-compilation process and the distributed boundaries
are always explicit. Using CORBA, the distributed part is written in IDL
and the core of the application is written in a host language.

@node Using A Distributed Language,  , Using A Middleware Environment, Introduction To Distributed Systems
@subsection Using A Distributed Language

Rather than defining a new language like an IDL, an alternative idea is
to extend a programming language in order to provide distributed
features. The distributed object paradigm provides a more
object-oriented approach to programming distributed systems. The notion
of a distributed object is an extension to the abstract data type that
permits the services provided in the type interface to be called
independently of where the actual service is executed. When combined
with object-oriented features such as inheritance and polymorphism,
distributed objects promote a more dynamic and structured computational
environment for distributed applications.

Ada95 includes a Distributed Systems Annex (DSA) which defines several
extensions allowing a user to write a distributed system entirely in
Ada, using Ada packages as the definition of remote procedure call or
remote method call on distributed objects. Ada95 distributed systems
model is close to Modula-3's one and Java/RMI's one is similar to
Ada95's one. In these languages, the IDL is replaced by a subset of the
language. Therefore, the language supports both remote procedure calls
and remote object method invocations transparently.

A program written in such a language is supposed to communicate with a
program written in the same language, but this restriction gives rise to
several useful consequences. The language can provide more powerful
features because it is not constrained by the common features available
in all host languages. In Ada95, the user will define a specification of
remote services and implement them exactly as he would for ordinary,
non-distributed services. His Ada95 environment will compile them to
produce a stub file (on the caller side) and a skeleton file that
automatically includes the services body (on the receiver
side). Creating objects, obtaining or registering object references or
adapting the object skeleton to the user object implementation are made
transparent because the language environment has a full control on the
development process.

Comparing with multi-thread programming once again, the language
extension solution is equivalent to the solution adopted for tasking
facilities in Ada.  Writing a distributed application is as simple as
writing a concurrent application: there is no binding consideration and
no code to wrap.  The language and its run-time system take care of
every issue that would divert the programmer's attention from the
application domain.

@node The Distributed System Annex, Architecture of A DSA Application, Introduction To Distributed Systems, Tutorial on the Distributed Systems Annex
@section Overview of Ada95 Distributed System Annex

The basic idea of the Distributed Systems Annex (DSA) is to allow a user
to develop his application the same way whether this application is
going to be executed as several programs on a distributed system or as a
whole program on a non-distributed system. The DSA has been designed so
as to minimize the changes required to the source code of a program, in
converting it from an ordinary non-distributed program, to a distributed
program.

The easiest way to start with DSA is certainly to develop the
application on a non-distributed system. Of course, the design of the
application should take into account the fact that some units are going
to be accessed remotely. In order to write an Ada95 distributed program,
it is necessary for the user to categorize certain library level
compilation units of the application program, by inserting
categorization pragmas into their specification. The units which require
categorization are typically those which are called remotely, or ones
which provide types used in remote calls.

Therefore, these units must contain only a restricted set of Ada
entities. For instance, if the distributed system has no shared memory,
shared variables should be forbidden. To ensure such restrictions, the
DSA provides several categorization pragmas in order to reduce the set
of entities one can declare in a given unit.

Of course, you can develop the non-distributed application with your
usual software engineering environment. This is very important to note
that the user needs no specialized tools to develop his/her distributed
application. For instance, he can debug his/her application with his/her
usual debugger. A non-distributed program is not to be confused with a
distributed application composed of only one program. The later is built
with the help of the configuration tool and includes the communication
library.

The last step is to partition and to configure the non-distributed
application into a distributed application, that means multiple
partitions working cooperatively as part of a single Ada program. The
process of mapping the partitions of a program to the nodes in a
distributed system is called configuring the partitions of the
program. This is what GLADE is for.

The distributed version of the user application should work as is, but
even when a program can be built either as a non-distributed or a
distributed program using the same source code, there may still be
differences in program execution between the distributed and
non-distributed versions. These differences will be presented in others
sections.

Developping a non-distributed application in order to distribute it
later on is the natural approach for a novice. Of course, it is not
always possible to write a distributed application as a non-distributed
application. For instance, a client/server application does not belong
to this category because several instances of the client can be active
at the same time. It is very easy to develop such an application using
GLADE and we shall present to the expert how to do that in the following
sections.

@node Architecture of A DSA Application, Presentation Of Categorization Pragmas, The Distributed System Annex, Tutorial on the Distributed Systems Annex
@section Architecture of A Distributed Ada95 Application

A distributed system is an interconnection of one or more processing
nodes and zero or more storage nodes. A distributed program comprises
one or more partitions. A partition is an aggregate of library
units. Partitions communicate through shared data or RPCs. A passive
partition has no thread of control. Only a passive partition can be
configured on a storage node. An active partition has zero or more
threads of control and has to be configured on a processing node.

The library unit is the core component of an Ada95 distributed
application. The user can explicitly assign library units to a
partition. The partitioning is a post-compilation process. The user
identifies at compile time interface packages. These packages are
categorized using pragmas. Each of these pragmas allows to use one of
the following classical paradigms:

@itemize @bullet
@item Remote subprograms:
For the programmer, a remote subprogram call is similar to a regular
subprogram call. Run-time binding using access-to-subprogram types can
also be used with remote subprograms. These remote subprograms are
mostly declared in library units categorized as remote call interface
(RCI).
  
@item Distributed objects:
Remote-pointer particular access types can be defined, which designate
remote objects. When a primitive dispatching operation is invoked on an
object designated by a remote access, a remote call is performed
transparently on the partition on which the object was created. These
distributed objects are declared in library units categorized as remote
types (RT).
  
@item Shared objects:
Global data can be shared between active partitions, providing a
repository similar to a shared memory, a shared file system or a
database. Entry-less protected objects allow safe access and update on
shared objects. This feature is orthogonal to the notion of distributed
objects, which are only accessed through exported services. These shared 
objects are declared in library units categorized as shared passive (SP).

@end itemize

The remotely-called subprograms declared in a library unit categorized
as remote call interface (RCI) or remote types (RT) may be either
statically or dynamically bound. The partition on which a statically
bound remote subprogram is executed can be determine before the
call. This is a regular remote subprogram call. A remote method or a
dereference of access to remote subprogram are dynamically bound remote
calls because the partition on which the remote subprogram is executed
is determined by the parameters of the call.

In the following example, Data_1 and Data_2 are shared passive (SP)
library units. Data_1 is configured on a passive partition mapped on a
storage node. Partition_1 and Partition_2 are active partitions. Note
that under some circumstances, a partition, for instance Partition_2,
can be duplicated. To be duplicated, Unit_2 and Unit_3 which are
configured on Partition_2 have to provide only dynamically bound remote
subprograms. Otherwise, a partition calling a remote subprogram on
Unit_2 would not be able to statically determine where to perform the
remote call.
 
@image{xe-arch.fig}

@node Presentation Of Categorization Pragmas, Pragma Remote_Call_Interface, Architecture of A DSA Application, Tutorial on the Distributed Systems Annex
@section Categorization Pragmas

Library units can be categorized according to the role they play in a
distributed program. A categorization pragma is a library unit pragma
that restricts declarations, child units or semantic dependences of the
library unit to which it applies. There are several categorization
pragmas :

@itemize @bullet
@item Remote_Call_Interface
@item Remote_Types
@item Shared_Passive
@item Pure
@end itemize

The following paragraphs do not present the detailed semantics of these
pragmas. Their purpose is to give to the reader an intuitive overview of
what these pragmas are for. When a library unit is not categorized, this
unit is called a normal unit and cannot play a role in the distributed
application. Such a unit is duplicated on any partition in which it is
involved.

As a general remark, to avoid the development of a specific run-time
library for the DSA, the notion of remote rendez-vous has not been
introduced in Ada95. Therefore, task types and general protected types
are not allowed in the following Ada library units.

@menu
* Pragma Remote_Call_Interface::  
* Pragma Remote_Types::         
* Transmitting Dynamic Structure::  
* Pragma Shared_Passive::       
* More About Pragmas::          
@end menu

@node Pragma Remote_Call_Interface, Pragma Remote_Types, Presentation Of Categorization Pragmas, Tutorial on the Distributed Systems Annex
@section Pragma Remote_Call_Interface

@menu
* Overview of Remote Call Interface Units::  
* Regular Remote Subprograms (RCI)::  
* Remote Access To Subprograms (RAS)::  
* Remote Access To Class Wide Types (RACW)::  
* Summary on Remote Call Interface Units::  
@end menu

@node  Overview of Remote Call Interface Units, Regular Remote Subprograms (RCI), Pragma Remote_Call_Interface, Pragma Remote_Call_Interface
@subsection Overview of Pragma Remote_Call_Interface

Library units categorized with this pragma can declare subprograms to be
called and executed remotely.  This classical RPC operation is a
statically bound operation. In these units, clients and servers do not
share their memory space.

Dynamically bound calls are integrated with Ada capabilities to
dereference subprograms (remote access to subprogram - RAS) and to
dispatch on access-to-class-wide operands (remote access on class wide
types - RACW). These remote access types can be declared in a RCI
package as well.

A remote access type (RAS or RACW) can be viewed as a fat pointer or a
structure with a remote address and a local address. The remote address
describes for instance the host on which the entity has been created;
the local address describes the classical local memory address (like an
URL).

@node Regular Remote Subprograms (RCI), Remote Access To Subprograms (RAS), Overview of Remote Call Interface Units, Pragma Remote_Call_Interface
@subsection Regular Remote Subprograms (RCI)

In the following example, a RCIBank offers several remote services:
Balance, Transfert, Deposit and Withdraw. On the caller side, the bank
client will use the stub files of unit RCIBank. On the receiver side,
the bank receiver will use the skeleton files of unit RCIBank including
the body of this package.

@image{types.ads}
@image{rcibank.ads}

@node Remote Access To Subprograms (RAS), Remote Access To Class Wide Types (RACW), Regular Remote Subprograms (RCI), Pragma Remote_Call_Interface
@subsection Remote Access To Subprograms (RAS)

In the following example, several mirroring banks offer their
services. Each bank registers a reference to each of its services to a
central bank. A central bank client asks for a service of one of the
mirroring banks. In this purpose, the RCI unit RASBank defines
Balance_Type, a remote access to subprogram. An access type in a remote
unit has to be either remote access to subprogram or remote access to
class wide type.

Note that to get a remote access to subprogram the subprogram has to be
remote itself. Therefore, MirrorBank is a RCI library unit.

@image{rasbank.ads}

In the code below, a mirroring bank registers its services to the
central bank.

@image{mirrorbank.ads}
@image{mirrorbank.adb}

In the code below, a central bank client asks for a mirroring bank and
calls the Balance service of this bank by dereferencing a remote access
type.

@image{bankclient.adb}

@menu
* Remote Access To Class Wide Types (RACW)::  
@end menu

@node Remote Access To Class Wide Types (RACW), Summary on Remote Call Interface Units, Remote Access To Subprograms (RAS), Pragma Remote_Call_Interface
@subsection Remote Access To Class Wide Types (RACW)

A bank client is now connected to a bank through a terminal. The bank
wants to notify a connected client with a message on its terminal when
another client grants him with a given amount of money.

@image{terminal.ads}

In the code below, the RCI unit RACWBank defines Term_Access, a remote
access to class wide type. Term_Access becomes a reference to a
distributed object. In the next section, we will see how to derive
Term_Type, how to create a distributed object and how to use a reference
to it.

@image{racwbank.ads}

@node  Summary on Remote Call Interface Units,  , Remote Access To Class Wide Types (RACW), Pragma Remote_Call_Interface
@subsection Summary on Pragma Remote_Call_Interface

Remote call interface units:

@itemize @bullet
@item
Allow subprograms to be called and executed remotely

@item
Allow statically bound remote calls (remote subprogram)

@item
Allow dynamically bound remote calls (remote access types)

@item
Forbid variables and access types

@item
Prevent specification from depending on normal units

@end itemize

@menu
* Pragma Remote_Types::         
* Pragma Shared_Passive::       
* More About Pragmas::          
@end menu

@node Pragma Remote_Types, Pragma Shared_Passive, Pragma Remote_Call_Interface, Tutorial on the Distributed Systems Annex
@section Pragma Remote_Types

@menu
* Overview of Remote Types Units::  
* Distributed Object::          
* Transmitting Dynamic Structure::  
* Summary on Remote Types Units::  
@end menu

@node  Overview of Remote Types Units, Distributed Object, Pragma Remote_Types, Pragma Remote_Types
@subsection Overview of Pragma Remote_Types

Unlike to RCI units, library units categorized with this pragma can
define distributed objects and remote methods on them. Both RCI and RT
units can define a remote access type described above (RACW). A
subprogram defined in a RT unit is not a remote subprogram. Unlike to
RCI units, a RT unit can be duplicated on several partitions in which
case all its entities are different with each other.

@node Distributed Object, Transmitting Dynamic Structure, Overview of Remote Types Units, Pragma Remote_Types
@subsection Distributed Object

If we want to implement the notification feature proposed in the
previous section, we have to derive Term_Type. Such a derivation is
possible in a remote types unit, NewTerminal. Any object of type
New_Term_Type becomes a distributed object and any reference to such an
object becomes a fat pointer or a reference to a distributed object.

@image{newterminal.ads}

In the code below, a client registers his terminal to
RACWBank. Therefore, when any donator grants him with a given amount of
money, RACWBank is able to notify this client of the granting operation.

@image{term2client.adb}

In the code below, a second client, the donator, registers his terminal
to the bank and executes a transfer to the first client.

@image{term1client.adb}

In the code below, we describe the general design of Transfer. Classical
operations of Withdraw and Deposit are performed. Then, RACWBank
retrieves the terminal of the granted client and invoke a dispatching
operation by dereferencing a distributed object Term. The reference is
analyzed and the execution of this operation occurs on the partition to
which the distributed object belongs.

@image{racwbank.adb}

@node Transmitting Dynamic Structure, Summary on Remote Types Units, Distributed Object, Pragma Remote_Types
@subsection Transmitting Dynamic Structure

@image{stringarraystream.ads}

General access types are forbidden in public part of a remote types
unit. But this would be too restrictive. It is possible to define
private general access types as long as the user provides its
marshalling procedures. The code below describes how to transmit a
linked structure.

The code below provides an implementation of the marshalling operations
defined above:

@image{stringarraystream.adb}

@menu
* Summary on Remote Types Units::  
@end menu

@node  Summary on Remote Types Units,  , Transmitting Dynamic Structure, Pragma Remote_Types
@subsection Summary on Remote Types Units

Remote types units:

@itemize @bullet
@item
Allow to define distributed objects

@item
Allow dynamically bound remote calls (remote access type)

@item
Allow general access type (with marshalling subprograms)

@item
Allow unit duplication (like normal units)

@item
Prevent specification from depending on normal units

@end itemize

@node Pragma Shared_Passive, More About Pragmas, Pragma Remote_Types, Tutorial on the Distributed Systems Annex
@section Pragma Shared_Passive

@menu
* Overview of Shared Passive Units::  
* Summary on Shared Passive Units::  
@end menu

@node  Overview of Shared Passive Units, Summary on Shared Passive Units, Pragma Shared_Passive, Pragma Shared_Passive
@subsection Overview of Pragma Shared_Passive

The entities declared in such categorized unit library are to be mapped
on a shared address space (file, memory, database). When two partitions
use such a library unit, they can communicate by reading or writing the
same variable. This supports the shared variables paradigm. Entry-less
protected objects declared in these units provide an atomic access to
some shared data, somehow like in a transaction.

@subsection Shared and Protected Objects

In the code below, we define two kinds of shared
objects. External_Synchronization requires that the different partitions
updating this data synchronize to avoid conflicting operations on shared
objects. Internal_Synchronization provides a way to get an atomic
operation on shared objects. Note that only entry-less subprograms are
allowed in a shared passive unit.

@image{sharedobjects.ads}

@node  Summary on Shared Passive Units,  , Overview of Shared Passive Units, Pragma Shared_Passive
@subsection Summary on Pragma Shared_Passive

@itemize @bullet
@item
Allow direct access to data among different partitions

@item
Allow support on shared (distributed) memory

@item
Allow memory protection use for entryless protected objects

@item
Prevent specification from depending on normal units

@end itemize

@node More About Pragmas, Most Features in One Example, Pragma Shared_Passive, Tutorial on the Distributed Systems Annex
@section More About Categorization Pragmas

@menu
* Variables::                   
* Pragma Asynchronous::         
* Pragma All_Calls_Remote::     
* Generic Categorized Units  ::  
* Categorization Unit Dependencies::  
@end menu

@node Variables, Pragma Asynchronous, More About Pragmas, More About Pragmas
@subsection Variables, Non-Remote Access Types, RPC Failures and Exceptions

In RT or RCI units, variables are forbidden and general access types are
allowed as long as their marshaling subprograms are provided. Calls are
made at most one time. They are made exactly one time or they fail with
an exception.  Any exception raised in remote method or subprogram call
is propagated to the caller. Exceptions semantics are preserved in the
regular Ada way.

@image{internal.ads}
@image{rempkg2.ads}
@image{rempkg1.ads}

Let's say that RemPkg2, Internal and RemExcMain packages are on the same
partition Partition_1 and that RemPkg1 is on partition Partition_2.

@image{rempkg2.adb}
@image{rempkg1.adb}
@image{remexcmain.adb}

When RemPkg1.Subprogram on Partition_1 raises Internal.Exc, this
exception is propagated on Partition_2. As Internal.Exc is not defined
on Partition_2, it is not possible to catch this exception without an
exception handler @b{when others}. When we reraise this exception in
RemPkg1.Subprogram, it is propagated on Partition_1. But this time,
Internal.Exc is defined and can be handled as we would in a regular Ada
program. Of course, the exception message is also preserved.
  
@node Pragma Asynchronous, Pragma All_Calls_Remote, Variables, More About Pragmas
@subsection Pragma Asynchronous

A pragma Asynchronous allows statically and dynamically bound remote
calls to be executed asynchronously. An asynchronous procedure doesn't
wait for the completion of the remote call and lets the caller continue
its execution path. This allows a calling task to proceed without
waiting for callee procedure to complete. The procedure must have only
@b{in} parameters and any exception raised during the execution of the
remote procedure is lost. Of course, this precludes exception
propagation of remotely called procedure.

When pragma Asynchronous applies to a regular subprogram with @b{in}
parameters, any call to this subprogram will be executed
asynchronously. Declaration of AsynchronousRCI.Asynchronous gives an
example.

@image{asynchronousrci.ads}
@image{asynchronousrt.ads}

A pragma Asynchronous applies to a RAS. An asynchronous RAS can be both
asynchronous and synchronous depending on the designated subprogram. For
install, in the code above, remote call (1) is asynchronous but remote
call (2) is synchronous.

A pragma Asynchronous applies to a RACW as well. In this case, the
invocation of @b{any} method with in parameters is @b{always} performed
asynchronously. Remote method invocation (3) is asynchronous when remote
method invocation (4) is synchronous.

@image{asynchronousmain.adb}

This feature allows message passing programming. But the user has to
realize that message passing programming, and asynchronous remote calls
in particular, has several drawbacks:

@itemize

@item
Violate original (remote) procedure semantics

@item
Allow some kind of remote GOTO mechanism

@item
Prevent development and debugging in a non-distributed context

@item
Introduce potential race conditions

@end itemize

To illustrate this, let's take the following example:

@image{node2.ads}
@image{node2.adb}
@image{node1.ads}
@image{node1.adb}
@image{nondeterministic.adb}

Let's say that Main is configured on Partition_0, Node1 on Partition_1
and Node2 on Partition_2. If Node1.Send and Node2.Send procedures were
synchronous or if no latency was introduced during network
communication, we would have the following RPC order: Main remotely
calls Node1.Send which remotely calls Node2.Send which sets V to
1. Then, Main remotely calls Node2.Send and sets V to 2.

Now, let's assume that both Send procedures are asynchronous and that
the connection between Partition_1 and Partition_2 is very slow. The
following scenario can very well occur. Main remotely calls Node1.Send
and is unblocked. Immediatly after this call, Main remotely calls
Node2.Send and sets V to 2. Once this is done, the remote call to
Node1.Send completes on Partition_1 and it remotely calls Node2.Send
which sets V to 1.

@node Pragma All_Calls_Remote, Generic Categorized Units  , Pragma Asynchronous, More About Pragmas
@subsection Pragma All_Calls_Remote

A pragma All_Calls_Remote in a RCI unit can force a remote procedure
call to be routed through the communication subsystem even for a local
call. This allows to debug an application in a non-distributed
situation that is very close to the distributed one.

@node Generic Categorized Units  , Categorization Unit Dependencies, Pragma All_Calls_Remote, More About Pragmas
@subsection Generic Categorized Units  

@image{genericrci.ads}
@image{rciinstantiation.ads}
@image{normalinstantiation.ads}

Any of these categorized units can be generic. Instances of these
generic packages can be either categorized or not. In the latter, such a
unit loses its categorization property. Like any categorized unit, the
generic categorized unit can only be instantiated at the library level
and regular restrictions of categorized units apply on instantiation (in
particular on generic formal parameters).

@node Categorization Unit Dependencies,  , Generic Categorized Units  , More About Pragmas
@subsection Categorization Unit Dependencies

Each categorization pragma has very specific visibility rules. As a
general rule, RCI > RT > SP > Pure. That means that a
Remote_Types package can make visible in its specification only
Remote_Types, Shared_Passive and Pure units.

@node Most Features in One Example, A Comparison between CORBA and DSA, More About Pragmas, Tutorial on the Distributed Systems Annex
@section Most Features in One Example

The example shown on the following figure highlights most of
the features of DSA. The general system is based on a set of factories
and workers and a storage.  Each entity is a partition itself. A
factory hires a worker from a pool of workers (hire - 1) and assigns a
job (query - 2) to him. The worker performs the job and saves the
result (reply - 3) in a storage common to all the factories.  The
worker notifies the factory of the end of his job (notify - 4).

@image{full-ex.fig}
@image{storage.ads}

When a worker has achieved his job, the result should be saved in a
common storage. To do this, we define a protected area in SP package
Storage (see sample above). Two entry-less protected objects
ensure atomic access on this area.

@image{common.ads}

Types is a Remote_Types package that defines most of the remote services
of the above system (see sample above). First, we define a way for the
workers to notify the end of his job.  This callback mechanism is
implemented using RAS Notify.

@image{newworkers.ads}

We define an abstract tagged type Worker which is intended to be the
root type of the whole distributed objects hierarchy. Assign allows a
factory to propose to a worker a job and a way to notify its employer
the end of this job. Any_Worker is a remote access to class wide type
(RACW). In other words, it is a reference to a distributed object of any
derived type from Worker class.

@image{newnewworkers.ads}

NewWorker is derived from type Worker and Assign is overridden. Sample
above shows how to derive a second generation of workers NewNewWorker
from the first generation NewWorker. As mentioned above, this RT package
can be duplicate on several partitions to produce several types of
workers and also several remote workers.

@image{workercity.ads}

In sample above, we define an unique place where workers wait for
jobs. Workers is a Remote_Call_Interface package with services to hire
and free workers. Unlike to Remote_Types packages, Remote_Call_Interface
packages cannot be duplicated.

@image{workercity.ads}
@image{newfactory.ads}

In order to use even more DSA features, Factory is defined as a generic
RCI package (see sample above). Any instantiation defines a new factory
(see sample above). To be RCI, this instantiation has to be categorized
once again.

@node A Comparison between CORBA and DSA,  , Most Features in One Example, Tutorial on the Distributed Systems Annex
@section A Comparison between CORBA and DSA

@menu
* CORBA Architecture::          
* Interface Definition Language::  
* Network Communication Subsystem::  
* Distributed Application Development::  
* Some elements of comparison::  
@end menu

@node  CORBA Architecture, Interface Definition Language, A Comparison between CORBA and DSA, A Comparison between CORBA and DSA
@subsection CORBA Architecture

CORBA is an industry-sponsored effort to standardize the distributed
object paradigm via the CORBA Interface Definition Language (IDL).  The
use of IDL makes CORBA more self-describing than any other client/server
middleware. The Common Object Request Broker: Architecture and
Specification, revision 2.2 describes the main features of CORBA which
are Interface Definition Language, Language Mappings, Stubs, Skeletons
and Object Adapters, ORB, Interface Repository, Dynamic Invocation, ORB
protocols and CORBA services.

@image{corba-arch.fig}

The IDL specifies modules, constants, types and interfaces. An object
interface defines the operations, exceptions and public attributes a
client can invoke or access. CORBA offers a model based only on
distributed objects. In some respects, it can be compared to Java as
this language provides only an object-oriented programming model, and
discards the classical structured programming model.

An IDL pre-compiler generates client stubs and server skeletons in a
host language (C++, C, Java, Smalltalk, Ada95); a language mapping
specifies how CORBA constructs are implemented in the host language. Of
course, if the host language is Java or C++, as the IDL is already
closed to these languages, the mapping is quite straightforward. If the
language is Ada95, then the user can have significantly much work to
suit the generated code to his actual needs.

The IDL pre-compiler produces two host language source files: a client
file called stub and a server file called skeleton. These files are
specific to a vendor and product, as they make calls to a proprietary
communication subsystem, but their structure is supposed to follow a
standard canvas.  The client stubs convert user queries into requests to
the ORB, which transmits these requests through an object adaptor to the
server skeleton.

@node Interface Definition Language, Network Communication Subsystem, CORBA Architecture, A Comparison between CORBA and DSA
@subsection Interface Definition Language

In DSA, the IDL is a subset of Ada95. The user identifies at compile
time interface packages. Some interface packages are categorized using
pragmas and these interface packages have to be unit libraries. There
are three kinds of categorization pragmas:

In CORBA, the IDL is a descriptive language; it supports @t{C++}
syntax for constant, type and operation declarations. From IDL
descriptions, a pre-compiler can directly generate client header files
and server implementation skeletons.

An IDL file can start by defining a module. This provides a name-space
to gather a set of interfaces. This is a way to introduce a level of
hierarchy (<@i{module}>::<@i{interface}>::<@i{operation}>). The Ada95
binding maps this element into a (child) package. @t{\#include}
will make any other namespaces visible.

A module can define interfaces. An interface defines a set of
methods that a client can invoke on an object. An interface can also
define exceptions and attributes. An exception is like a @t{C++}
exception and a component can be attached to it. An attribute is a
component field. For each of them, the implementation automatically
creates Get and Set operations. Only Get is provided for
readonly attributes. An interface can derived from one or more
interfaces (multiple inheritance).

The Ada95 binding maps this element into a package or a child
package. For the client stub, the implementation will automatically
create a tagged type named Ref (which is derived from CORBA.Object.Ref
or from the derived type Ref of another interface) in a package named
like the interface. For the server skeleton, the implementation will
automatically create a tagged type named Object (which is derived from
an implementation defined private tagged type Object) in a package named
Impl, child package of a package named like the interface
(<@i{interface}>.Impl).

@image{naming.idl}

A method is defined by its unique name (no overloading allowed) and its
signature (the types of its formal parameters). Each parameter can be of
mode @b{in}, @b{out} or @b{inout}, whose meaning is
comparable to their Ada homonyms. Every exception that can be raised by
a method must also be declared as part of the method signature.

In addition, the @b{oneway} attribute can be applied to a subprogram,
making it an asynchronous method. The caller of an asynchronous method
will continue its execution path without waiting for the completion of
the remote call.

Most CORBA data types map naturally onto predefined Ada types, with
the exception of @t{any} and @t{sequence}. @t{any},
that can designate any CORBA type, will be mapped onto a stream type
with @t{read} and @t{write} operations. A @t{sequence}
holds a list of data with a given type and will be represented in Ada
using a pair of lengthy generic packages (the declaration itself would
take about 20 pages). One may note that the CORBA @t{string} type
is mapped onto the @t{Unbounded\_String} predefined Ada type.

The Ada95 mapping provides special mechanisms to implement two difficult
CORBA features. First, it provides a way to implement multiple
inheritance. As described above, an Ada95 package defines a type derived
from the first interface and extends the list of its primitives to
achieve inheritance from other interfaces. Another difficult feature of
CORBA comes from forward declarations. In Ada, two package
specifications cannot ``with'' each others, but this can occur between
to IDL interfaces (see IDL specification above). To solve this, the
mapping proposes to create ``forward'' packages. This can result in a
very non-intuitive situation where the client stub does not ``with'' its
usual interface packages but ``forward'' packages.

We can note that to develop a distributed application with CORBA, a good
understanding of the IDL is not enough. The user has also to understand
very well the language mapping defined for the host language he wants to
use. This can introduce a high level of complexity in the user code.

Interface information coded in IDL can be stored in an on-line database
of object definitions called Interface Repository. A CORBA specification
describes how the interface repository is organized and how to retrieve
information on IDL from this repository. The reader familiar with Ada95
tools will note that this information is very close to what ASIS would
provide.

This interface repository becomes useful when a client wants to invoke a
method for which he does not know the signature. CORBA defines an API to
retrieve in the database the data that defines the server operation. It
also defines a way to build a full request by generating the parameters
needed for a method invocation.

Basically, this API allows the client to explore the repository classes
to obtain a module definition tree. From this tree, the client is able
to extract subtrees defining constants, types, exceptions and
interfaces. From an interface subtree, the client can select an
operation with its list of parameters (type, name and mode) and
exceptions.

Therefore, the client has three ways to invoke the request. He can send
the request and wait for the results. But he can also defer to get the
result or ask to ignore the results (oneway). This mechanism is known as
Dynamic Invocation Interface (DII). This mechanism is also available on
the server side and is known as Dynamic Skeleton Interface. Of course,
these mechanisms are powerful but very complex and tedious to use. Most
of the users will keep working with static invocations.

@node Network Communication Subsystem, Distributed Application Development, Interface Definition Language, A Comparison between CORBA and DSA
@subsection Network Communication Subsystem

@menu
* DSA PCS::                     
* CORBA ORB::                   
@end menu

@node DSA PCS, CORBA ORB, Network Communication Subsystem, Network Communication Subsystem
@subsubsection DSA PCS

In the DSA world, everything that is not done by the compiler in regard
to the distribution belongs to the partition communication subsystem
(PCS). For example figuring out on which partition a package that will
be called remotely is located is part of the PCS responsibility.

The PCS entry points are well defined in the Distributed Systems Annex
and described in the @t{System.RPC} package declaration.  By looking at
this package, one can notice that there is nothing related to abortion
of remote subprogram calls, although the Annex states that if such a
call is aborted, an abortion message must be sent to the remote
partition to cancel remote processing. That means that the PCS is in
charge of detecting that a call to one of its entry points has been
aborted and must send such an abortion message, without any help from
the compiler.

Another interesting characteristic of the PCS is its behavior regarding
unknown exceptions. When an exception is raised as the result of the
execution of a remote subprogram call, it is propagated back to the
caller. However, the caller may not have any visibility over the
exception declaration, but may still catch it with a @t{when
  others} clause. But if the caller does not catch it and let it be
propagated upstream (maybe in another partition), and if the upstream
caller has visibility over this exception, it must be able to catch it
using its name. That means that the PCS must recognize that a
previously unknown exception maps onto a locally known one, for
example by being able to dynamically register a new exception into the 
runtime.

@node CORBA ORB,  , DSA PCS, Network Communication Subsystem
@subsubsection CORBA ORB

CORBA has adopted a much more fragmented point of view in regard to
services: as much services as possible will be defined externally.  For
example, the naming service (whose duty is to locate remote objects from
their name) is itself a distributed object with a standardized IDL
interface.

While this looks like more pure an approach, this may also induce some
slowdowns. Being itself a distributed object, the naming service cannot
be optimized specifically for the ORB needs.  Also some magic is
required in the ORB to be able to locate the naming service itself
(chicken and egg problem).

Regarding exception propagation, an ORB is not able to propagate an
exception that has not been declared in the IDL interface. This
restriction, although annoying because it restricts the usage of
exceptions, is understandable given the multi-language CORBA approach:
what should be done for example with a C++ exception which reaches a
caller written in Ada?

@node Distributed Application Development, Some elements of comparison, Network Communication Subsystem, A Comparison between CORBA and DSA
@subsection Distributed Application Development

@menu
* DSA Application Development::  
* CORBA Application Development::  
@end menu

@node DSA Application Development, CORBA Application Development, Distributed Application Development, Distributed Application Development
@subsubsection DSA Application Development

The DSA does not describe how a distributed application should be
configured. It is up to the user (using a partitioning tool whose
specification is outside the scope of the annex) to define what the
partitions in her program are and on which machines they should be
executed.

GLADE provides a Configuration Tool and a Partition Communication
Subsystem to build a distributed application. The GNATDIST tool and its
configuration language have been purposely designed to let the user
partition her program and specify the machines where the individual
partitions will be executing. The Generic Ada Reusable Library for
Interpartition Communication (GARLIC) is a high level communication
library that implements with object-oriented techniques the interface
between the Partition Communication Subsystem defined in the Reference
Manual and the network communication layer.

@node CORBA Application Development,  , DSA Application Development, Distributed Application Development
@subsubsection CORBA Application Development

@c \reviewers{We shall give more details on the distributed
@c   application development in the final paper. This will also describe
@c   the services available in CORBA: CosNaming, CosEvents,
@c   ...}

@node Some elements of comparison,  , Distributed Application Development, A Comparison between CORBA and DSA
@subsection Some elements of comparison

CORBA provides an outstanding and very popular framework. The IDL syntax
is close to @t{C++}. The object model is close to @t{Java}: CORBA
defines only distributed objects. Furthermore, the stub and skeleton
generated code is close to Java with two root classes, Ref for clients
and Object for servers.

DSA provides a more general model. This includes distributed objects,
but also regular remote subprograms and references on remote
subprograms. Shared passive packages can be defined as an abstraction
for a (distributed) shared memory, a persistency support or a database.
Basically, the IDL is a subset of Ada95 and the remote services are
defined in packages categorized by three kinds of pragmas (RCI, RT,
SP). The distributed boundaries are more transparent as the application
is not split into IDL sources and others sources in some host languages.

To use a client stub or server skeleton, a CORBA user has to perfectly
understand the Ada95 mapping or any other language mapping. Some types
can be quite difficult or expensive to map like exceptions, any, string
and sequence. Some IDL features are also difficult to implement in
Ada95 like multiple inheritance and forward declarations.

In DSA, any Ada type can be used except access types, but this can be
solved by providing the marshaling operations for such a type. The
exception model is entirely preserved. Overloading is allowed in DSA
(not in CORBA). The user can also define generic packages and use mixin
mechanism to obtain some kind of multiple inheritance.

The DSA user can design, implement and test his application in a
non-distributed environment, and then switch to a distributed situation.
With this two-phase design approach, the user always works within his
favorite Ada95 environment. Pragma All_Calls_Remote also facilitates
debugging of a distributed application in a non-distributed context.

The CORBA user has to re-adapt his code to the code generated by the
pre-compiler from the IDL file anytime it is modified. He also has to
use the predefined CORBA types instead of Ada standard types; he has to
call adaptor services to obtain remote object references or a specific
naming service.

As Ada95 is the IDL, the user will not have to deal with any generated
stub or skeleton code. The configuration environment will take care of
updating object, stub and skeleton files when sources have been
updated. The system will automatically take in charge some naming
services like declaring RCI services. It will also take care of aborting
remote procedure calls, detecting the distributed termination, checking
version consistency between clients and servers or preserving and
propagating remote exceptions.

CORBA is a very rich but very complex system. The drawbacks include the
high learning curve for developing and managing CORBA applications
effectively, performance limitations, as well as the lack of portability
and security.  These drawbacks are the price to pay for language
interoperability, a facility the Ada95-oriented DSA does not provide.

Using its IDL, the OMG has described a number of @i{Common Object
Services} that are frequently needed to develop distributed
systems. Unfortunately, these specifications are limited to an IDL
description, and most of the semantics is up to the vendor. The DSA
misses such a user-level library including basic distributed software
components.  More generally, the lack of component libraries has always
been a problem for Ada.

@node Getting Started With GLADE, Index, Tutorial on the Distributed Systems Annex, Top
@chapter Getting Started With GLADE

This chapter describes the usual ways of using GLADE to compile Ada
distributed programs.

@node Index,  , Getting Started With GLADE, Top
@unnumbered Index

@printindex cp

@contents

@bye
